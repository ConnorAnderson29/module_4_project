{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_keras.transformer.load import load_google_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_google_bert('BERT_keras/wwm_cased_L-24_H-1024_A-16/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 512, 1024)    2048        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 512, 1024)    524288      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 512, 1024)    29591552    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 512, 1024)    0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512, 1024)    2048        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 512, 1024)    0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 512, 3072)    3148800     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 1, 512, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 512, 1024)    0           layer_0/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 512, 1024)    0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 512, 4096)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 512, 1024)    0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 512, 1024)    0           layer_1/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 512, 1024)    0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 512, 4096)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 512, 1024)    0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 512, 1024)    0           layer_2/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 512, 1024)    0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 512, 4096)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 512, 1024)    0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 512, 1024)    0           layer_3/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 512, 1024)    0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 512, 4096)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 512, 1024)    0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 512, 1024)    0           layer_4/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 512, 1024)    0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 512, 4096)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 512, 1024)    0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 512, 1024)    0           layer_5/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 512, 1024)    0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 512, 4096)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 512, 1024)    0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 512, 1024)    0           layer_6/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 512, 1024)    0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 512, 4096)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 512, 1024)    0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 512, 1024)    0           layer_7/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 512, 1024)    0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 512, 4096)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 512, 1024)    0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 512, 1024)    0           layer_8/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 512, 1024)    0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 512, 4096)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 512, 1024)    0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 512, 1024)    0           layer_9/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 512, 1024)    0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 512, 4096)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 512, 1024)    0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 512, 1024)    0           layer_10/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 512, 1024)    0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 512, 4096)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 512, 1024)    0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 512, 1024)    0           layer_11/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 512, 1024)    0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 512, 4096)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 512, 1024)    0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/self_attention (MultiH (None, 512, 1024)    0           layer_12/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_12/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_add (Add)         (None, 512, 1024)    0           layer_11/ln_2[0][0]              \n",
      "                                                                 layer_12/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_12/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/gelu (Gelu)            (None, 512, 4096)    0           layer_12/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_12/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_add (Add)         (None, 512, 1024)    0           layer_12/ln_1[0][0]              \n",
      "                                                                 layer_12/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_12/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/self_attention (MultiH (None, 512, 1024)    0           layer_13/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_13/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_add (Add)         (None, 512, 1024)    0           layer_12/ln_2[0][0]              \n",
      "                                                                 layer_13/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_13/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/gelu (Gelu)            (None, 512, 4096)    0           layer_13/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_13/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_add (Add)         (None, 512, 1024)    0           layer_13/ln_1[0][0]              \n",
      "                                                                 layer_13/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_13/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/self_attention (MultiH (None, 512, 1024)    0           layer_14/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_14/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_add (Add)         (None, 512, 1024)    0           layer_13/ln_2[0][0]              \n",
      "                                                                 layer_14/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_14/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/gelu (Gelu)            (None, 512, 4096)    0           layer_14/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_14/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_add (Add)         (None, 512, 1024)    0           layer_14/ln_1[0][0]              \n",
      "                                                                 layer_14/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_14/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/self_attention (MultiH (None, 512, 1024)    0           layer_15/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_15/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_add (Add)         (None, 512, 1024)    0           layer_14/ln_2[0][0]              \n",
      "                                                                 layer_15/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_15/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/gelu (Gelu)            (None, 512, 4096)    0           layer_15/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_15/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_add (Add)         (None, 512, 1024)    0           layer_15/ln_1[0][0]              \n",
      "                                                                 layer_15/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_15/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/self_attention (MultiH (None, 512, 1024)    0           layer_16/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_16/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_add (Add)         (None, 512, 1024)    0           layer_15/ln_2[0][0]              \n",
      "                                                                 layer_16/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_16/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/gelu (Gelu)            (None, 512, 4096)    0           layer_16/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_16/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_add (Add)         (None, 512, 1024)    0           layer_16/ln_1[0][0]              \n",
      "                                                                 layer_16/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_16/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/self_attention (MultiH (None, 512, 1024)    0           layer_17/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_17/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_add (Add)         (None, 512, 1024)    0           layer_16/ln_2[0][0]              \n",
      "                                                                 layer_17/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_17/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/gelu (Gelu)            (None, 512, 4096)    0           layer_17/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_17/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_add (Add)         (None, 512, 1024)    0           layer_17/ln_1[0][0]              \n",
      "                                                                 layer_17/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_17/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/self_attention (MultiH (None, 512, 1024)    0           layer_18/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_18/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_add (Add)         (None, 512, 1024)    0           layer_17/ln_2[0][0]              \n",
      "                                                                 layer_18/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_18/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/gelu (Gelu)            (None, 512, 4096)    0           layer_18/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_18/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_add (Add)         (None, 512, 1024)    0           layer_18/ln_1[0][0]              \n",
      "                                                                 layer_18/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_18/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/self_attention (MultiH (None, 512, 1024)    0           layer_19/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_19/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_add (Add)         (None, 512, 1024)    0           layer_18/ln_2[0][0]              \n",
      "                                                                 layer_19/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_19/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/gelu (Gelu)            (None, 512, 4096)    0           layer_19/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_19/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_add (Add)         (None, 512, 1024)    0           layer_19/ln_1[0][0]              \n",
      "                                                                 layer_19/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_19/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/self_attention (MultiH (None, 512, 1024)    0           layer_20/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_20/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_add (Add)         (None, 512, 1024)    0           layer_19/ln_2[0][0]              \n",
      "                                                                 layer_20/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_20/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/gelu (Gelu)            (None, 512, 4096)    0           layer_20/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_20/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_add (Add)         (None, 512, 1024)    0           layer_20/ln_1[0][0]              \n",
      "                                                                 layer_20/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_20/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/self_attention (MultiH (None, 512, 1024)    0           layer_21/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_21/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_add (Add)         (None, 512, 1024)    0           layer_20/ln_2[0][0]              \n",
      "                                                                 layer_21/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_21/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/gelu (Gelu)            (None, 512, 4096)    0           layer_21/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_21/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_add (Add)         (None, 512, 1024)    0           layer_21/ln_1[0][0]              \n",
      "                                                                 layer_21/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_21/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/self_attention (MultiH (None, 512, 1024)    0           layer_22/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_22/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_add (Add)         (None, 512, 1024)    0           layer_21/ln_2[0][0]              \n",
      "                                                                 layer_22/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_22/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/gelu (Gelu)            (None, 512, 4096)    0           layer_22/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_22/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_add (Add)         (None, 512, 1024)    0           layer_22/ln_1[0][0]              \n",
      "                                                                 layer_22/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_22/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/self_attention (MultiH (None, 512, 1024)    0           layer_23/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_23/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_add (Add)         (None, 512, 1024)    0           layer_22/ln_2[0][0]              \n",
      "                                                                 layer_23/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_23/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/gelu (Gelu)            (None, 512, 4096)    0           layer_23/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_23/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_add (Add)         (None, 512, 1024)    0           layer_23/ln_1[0][0]              \n",
      "                                                                 layer_23/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_2_add[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 332,429,312\n",
      "Trainable params: 332,429,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.trainable=False\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in bert_model.layers:\n",
    "#     model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 512, 1024)    2048        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 512, 1024)    524288      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 512, 1024)    29591552    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 512, 1024)    0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512, 1024)    2048        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 512, 1024)    0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 512, 3072)    3148800     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 1, 512, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 512, 1024)    0           layer_0/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 512, 1024)    0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 512, 4096)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 512, 1024)    0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 512, 1024)    0           layer_1/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 512, 1024)    0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 512, 4096)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 512, 1024)    0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 512, 1024)    0           layer_2/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 512, 1024)    0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 512, 4096)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 512, 1024)    0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 512, 1024)    0           layer_3/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 512, 1024)    0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 512, 4096)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 512, 1024)    0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 512, 1024)    0           layer_4/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 512, 1024)    0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 512, 4096)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 512, 1024)    0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 512, 1024)    0           layer_5/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 512, 1024)    0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 512, 4096)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 512, 1024)    0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 512, 1024)    0           layer_6/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 512, 1024)    0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 512, 4096)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 512, 1024)    0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 512, 1024)    0           layer_7/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 512, 1024)    0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 512, 4096)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 512, 1024)    0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 512, 1024)    0           layer_8/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 512, 1024)    0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 512, 4096)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 512, 1024)    0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 512, 1024)    0           layer_9/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 512, 1024)    0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 512, 4096)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 512, 1024)    0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 512, 1024)    0           layer_10/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 512, 1024)    0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 512, 4096)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 512, 1024)    0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 512, 1024)    0           layer_11/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 512, 1024)    0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 512, 4096)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 512, 1024)    0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/self_attention (MultiH (None, 512, 1024)    0           layer_12/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_12/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_add (Add)         (None, 512, 1024)    0           layer_11/ln_2[0][0]              \n",
      "                                                                 layer_12/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_12/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/gelu (Gelu)            (None, 512, 4096)    0           layer_12/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_12/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_add (Add)         (None, 512, 1024)    0           layer_12/ln_1[0][0]              \n",
      "                                                                 layer_12/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_12/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/self_attention (MultiH (None, 512, 1024)    0           layer_13/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_13/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_add (Add)         (None, 512, 1024)    0           layer_12/ln_2[0][0]              \n",
      "                                                                 layer_13/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_13/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/gelu (Gelu)            (None, 512, 4096)    0           layer_13/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_13/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_add (Add)         (None, 512, 1024)    0           layer_13/ln_1[0][0]              \n",
      "                                                                 layer_13/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_13/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/self_attention (MultiH (None, 512, 1024)    0           layer_14/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_14/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_add (Add)         (None, 512, 1024)    0           layer_13/ln_2[0][0]              \n",
      "                                                                 layer_14/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_14/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/gelu (Gelu)            (None, 512, 4096)    0           layer_14/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_14/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_add (Add)         (None, 512, 1024)    0           layer_14/ln_1[0][0]              \n",
      "                                                                 layer_14/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_14/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/self_attention (MultiH (None, 512, 1024)    0           layer_15/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_15/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_add (Add)         (None, 512, 1024)    0           layer_14/ln_2[0][0]              \n",
      "                                                                 layer_15/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_15/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/gelu (Gelu)            (None, 512, 4096)    0           layer_15/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_15/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_add (Add)         (None, 512, 1024)    0           layer_15/ln_1[0][0]              \n",
      "                                                                 layer_15/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_15/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/self_attention (MultiH (None, 512, 1024)    0           layer_16/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_16/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_add (Add)         (None, 512, 1024)    0           layer_15/ln_2[0][0]              \n",
      "                                                                 layer_16/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_16/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/gelu (Gelu)            (None, 512, 4096)    0           layer_16/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_16/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_add (Add)         (None, 512, 1024)    0           layer_16/ln_1[0][0]              \n",
      "                                                                 layer_16/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_16/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/self_attention (MultiH (None, 512, 1024)    0           layer_17/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_17/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_add (Add)         (None, 512, 1024)    0           layer_16/ln_2[0][0]              \n",
      "                                                                 layer_17/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_17/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/gelu (Gelu)            (None, 512, 4096)    0           layer_17/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_17/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_add (Add)         (None, 512, 1024)    0           layer_17/ln_1[0][0]              \n",
      "                                                                 layer_17/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_17/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/self_attention (MultiH (None, 512, 1024)    0           layer_18/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_18/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_add (Add)         (None, 512, 1024)    0           layer_17/ln_2[0][0]              \n",
      "                                                                 layer_18/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_18/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/gelu (Gelu)            (None, 512, 4096)    0           layer_18/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_18/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_add (Add)         (None, 512, 1024)    0           layer_18/ln_1[0][0]              \n",
      "                                                                 layer_18/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_18/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/self_attention (MultiH (None, 512, 1024)    0           layer_19/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_19/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_add (Add)         (None, 512, 1024)    0           layer_18/ln_2[0][0]              \n",
      "                                                                 layer_19/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_19/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/gelu (Gelu)            (None, 512, 4096)    0           layer_19/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_19/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_add (Add)         (None, 512, 1024)    0           layer_19/ln_1[0][0]              \n",
      "                                                                 layer_19/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_19/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/self_attention (MultiH (None, 512, 1024)    0           layer_20/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_20/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_add (Add)         (None, 512, 1024)    0           layer_19/ln_2[0][0]              \n",
      "                                                                 layer_20/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_20/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/gelu (Gelu)            (None, 512, 4096)    0           layer_20/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_20/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_add (Add)         (None, 512, 1024)    0           layer_20/ln_1[0][0]              \n",
      "                                                                 layer_20/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_20/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/self_attention (MultiH (None, 512, 1024)    0           layer_21/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_21/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_add (Add)         (None, 512, 1024)    0           layer_20/ln_2[0][0]              \n",
      "                                                                 layer_21/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_21/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/gelu (Gelu)            (None, 512, 4096)    0           layer_21/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_21/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_add (Add)         (None, 512, 1024)    0           layer_21/ln_1[0][0]              \n",
      "                                                                 layer_21/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_21/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/self_attention (MultiH (None, 512, 1024)    0           layer_22/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_22/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_add (Add)         (None, 512, 1024)    0           layer_21/ln_2[0][0]              \n",
      "                                                                 layer_22/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_22/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/gelu (Gelu)            (None, 512, 4096)    0           layer_22/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_22/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_add (Add)         (None, 512, 1024)    0           layer_22/ln_1[0][0]              \n",
      "                                                                 layer_22/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_22/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/self_attention (MultiH (None, 512, 1024)    0           layer_23/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_23/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_add (Add)         (None, 512, 1024)    0           layer_22/ln_2[0][0]              \n",
      "                                                                 layer_23/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_23/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/gelu (Gelu)            (None, 512, 4096)    0           layer_23/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_23/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_add (Add)         (None, 512, 1024)    0           layer_23/ln_1[0][0]              \n",
      "                                                                 layer_23/ln_2_drop[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 332,427,264\n",
      "Trainable params: 0\n",
      "Non-trainable params: 332,427,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_model.build(None)\n",
    "bert_model._layers.pop()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-522a83368298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "bert_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
