{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_keras.transformer.load import load_google_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BERT_keras.google_bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "This is a test; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-09e51793e826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google Drive/flatiron_school/final_projects/module_4_project/BERT_keras/google_bert/tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/flatiron_school/final_projects/module_4_project/BERT_keras/google_bert/tokenization.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m       \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line from the file. Leaves the '\\n' at the end.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadLineAsString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 85\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/learn-env/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: This is a test; No such file or directory"
     ]
    }
   ],
   "source": [
    "tokened = FullTokenizer('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = load_google_bert('BERT_keras/wwm_cased_L-24_H-1024_A-16/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 512, 1024)    2048        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 512, 1024)    524288      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 512, 1024)    29591552    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 512, 1024)    0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512, 1024)    2048        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 512, 1024)    0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 512, 3072)    3148800     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 1, 512, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 512, 1024)    0           layer_0/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 512, 1024)    0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 512, 4096)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 512, 1024)    0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 512, 1024)    0           layer_1/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 512, 1024)    0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 512, 4096)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 512, 1024)    0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 512, 1024)    0           layer_2/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 512, 1024)    0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 512, 4096)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 512, 1024)    0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 512, 1024)    0           layer_3/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 512, 1024)    0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 512, 4096)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 512, 1024)    0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 512, 1024)    0           layer_4/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 512, 1024)    0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 512, 4096)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 512, 1024)    0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 512, 1024)    0           layer_5/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 512, 1024)    0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 512, 4096)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 512, 1024)    0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 512, 1024)    0           layer_6/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 512, 1024)    0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 512, 4096)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 512, 1024)    0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 512, 1024)    0           layer_7/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 512, 1024)    0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 512, 4096)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 512, 1024)    0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 512, 1024)    0           layer_8/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 512, 1024)    0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 512, 4096)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 512, 1024)    0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 512, 1024)    0           layer_9/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 512, 1024)    0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 512, 4096)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 512, 1024)    0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 512, 1024)    0           layer_10/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 512, 1024)    0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 512, 4096)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 512, 1024)    0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 512, 1024)    0           layer_11/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 512, 1024)    0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 512, 4096)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 512, 1024)    0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/self_attention (MultiH (None, 512, 1024)    0           layer_12/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_12/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_add (Add)         (None, 512, 1024)    0           layer_11/ln_2[0][0]              \n",
      "                                                                 layer_12/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_12/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/gelu (Gelu)            (None, 512, 4096)    0           layer_12/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_12/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_add (Add)         (None, 512, 1024)    0           layer_12/ln_1[0][0]              \n",
      "                                                                 layer_12/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_12/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/self_attention (MultiH (None, 512, 1024)    0           layer_13/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_13/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_add (Add)         (None, 512, 1024)    0           layer_12/ln_2[0][0]              \n",
      "                                                                 layer_13/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_13/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/gelu (Gelu)            (None, 512, 4096)    0           layer_13/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_13/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_add (Add)         (None, 512, 1024)    0           layer_13/ln_1[0][0]              \n",
      "                                                                 layer_13/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_13/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/self_attention (MultiH (None, 512, 1024)    0           layer_14/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_14/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_add (Add)         (None, 512, 1024)    0           layer_13/ln_2[0][0]              \n",
      "                                                                 layer_14/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_14/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/gelu (Gelu)            (None, 512, 4096)    0           layer_14/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_14/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_add (Add)         (None, 512, 1024)    0           layer_14/ln_1[0][0]              \n",
      "                                                                 layer_14/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_14/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/self_attention (MultiH (None, 512, 1024)    0           layer_15/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_15/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_add (Add)         (None, 512, 1024)    0           layer_14/ln_2[0][0]              \n",
      "                                                                 layer_15/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_15/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/gelu (Gelu)            (None, 512, 4096)    0           layer_15/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_15/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_add (Add)         (None, 512, 1024)    0           layer_15/ln_1[0][0]              \n",
      "                                                                 layer_15/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_15/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/self_attention (MultiH (None, 512, 1024)    0           layer_16/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_16/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_add (Add)         (None, 512, 1024)    0           layer_15/ln_2[0][0]              \n",
      "                                                                 layer_16/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_16/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/gelu (Gelu)            (None, 512, 4096)    0           layer_16/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_16/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_add (Add)         (None, 512, 1024)    0           layer_16/ln_1[0][0]              \n",
      "                                                                 layer_16/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_16/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/self_attention (MultiH (None, 512, 1024)    0           layer_17/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_17/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_add (Add)         (None, 512, 1024)    0           layer_16/ln_2[0][0]              \n",
      "                                                                 layer_17/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_17/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/gelu (Gelu)            (None, 512, 4096)    0           layer_17/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_17/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_add (Add)         (None, 512, 1024)    0           layer_17/ln_1[0][0]              \n",
      "                                                                 layer_17/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_17/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/self_attention (MultiH (None, 512, 1024)    0           layer_18/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_18/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_add (Add)         (None, 512, 1024)    0           layer_17/ln_2[0][0]              \n",
      "                                                                 layer_18/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_18/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/gelu (Gelu)            (None, 512, 4096)    0           layer_18/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_18/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_add (Add)         (None, 512, 1024)    0           layer_18/ln_1[0][0]              \n",
      "                                                                 layer_18/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_18/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/self_attention (MultiH (None, 512, 1024)    0           layer_19/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_19/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_add (Add)         (None, 512, 1024)    0           layer_18/ln_2[0][0]              \n",
      "                                                                 layer_19/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_19/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/gelu (Gelu)            (None, 512, 4096)    0           layer_19/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_19/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_add (Add)         (None, 512, 1024)    0           layer_19/ln_1[0][0]              \n",
      "                                                                 layer_19/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_19/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/self_attention (MultiH (None, 512, 1024)    0           layer_20/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_20/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_add (Add)         (None, 512, 1024)    0           layer_19/ln_2[0][0]              \n",
      "                                                                 layer_20/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_20/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/gelu (Gelu)            (None, 512, 4096)    0           layer_20/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_20/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_add (Add)         (None, 512, 1024)    0           layer_20/ln_1[0][0]              \n",
      "                                                                 layer_20/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_20/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/self_attention (MultiH (None, 512, 1024)    0           layer_21/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_21/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_add (Add)         (None, 512, 1024)    0           layer_20/ln_2[0][0]              \n",
      "                                                                 layer_21/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_21/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/gelu (Gelu)            (None, 512, 4096)    0           layer_21/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_21/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_add (Add)         (None, 512, 1024)    0           layer_21/ln_1[0][0]              \n",
      "                                                                 layer_21/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_21/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/self_attention (MultiH (None, 512, 1024)    0           layer_22/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_22/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_add (Add)         (None, 512, 1024)    0           layer_21/ln_2[0][0]              \n",
      "                                                                 layer_22/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_22/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/gelu (Gelu)            (None, 512, 4096)    0           layer_22/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_22/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_add (Add)         (None, 512, 1024)    0           layer_22/ln_1[0][0]              \n",
      "                                                                 layer_22/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_22/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/self_attention (MultiH (None, 512, 1024)    0           layer_23/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_23/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_add (Add)         (None, 512, 1024)    0           layer_22/ln_2[0][0]              \n",
      "                                                                 layer_23/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_23/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/gelu (Gelu)            (None, 512, 4096)    0           layer_23/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_23/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_add (Add)         (None, 512, 1024)    0           layer_23/ln_1[0][0]              \n",
      "                                                                 layer_23/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_2_add[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 332,429,312\n",
      "Trainable params: 332,429,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.trainable=False\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in bert_model.layers:\n",
    "#     model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "segment_input (InputLayer)      (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "position_input (InputLayer)     (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "SegmentEmbedding (Embedding)    (None, 512, 1024)    2048        segment_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "PositionEmbedding (Embedding)   (None, 512, 1024)    524288      position_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "TokenEmbedding (Embedding)      (None, 512, 1024)    29591552    token_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "AddEmbeddings (Add)             (None, 512, 1024)    0           SegmentEmbedding[0][0]           \n",
      "                                                                 PositionEmbedding[0][0]          \n",
      "                                                                 TokenEmbedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 512, 1024)    2048        AddEmbeddings[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "EmbeddingDropOut (Dropout)      (None, 512, 1024)    0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn (Conv1D)         (None, 512, 3072)    3148800     EmbeddingDropOut[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask_input (InputLaye (None, 1, 512, 512)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/self_attention (MultiHe (None, 512, 1024)    0           layer_0/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_0/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1_add (Add)          (None, 512, 1024)    0           EmbeddingDropOut[0][0]           \n",
      "                                                                 layer_0/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_0/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/gelu (Gelu)             (None, 512, 4096)    0           layer_0/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_0/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_0/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2_add (Add)          (None, 512, 1024)    0           layer_0/ln_1[0][0]               \n",
      "                                                                 layer_0/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_0/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_0/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_0/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/self_attention (MultiHe (None, 512, 1024)    0           layer_1/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_1/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1_add (Add)          (None, 512, 1024)    0           layer_0/ln_2[0][0]               \n",
      "                                                                 layer_1/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_1/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/gelu (Gelu)             (None, 512, 4096)    0           layer_1/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_1/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_1/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2_add (Add)          (None, 512, 1024)    0           layer_1/ln_1[0][0]               \n",
      "                                                                 layer_1/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_1/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_1/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_1/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/self_attention (MultiHe (None, 512, 1024)    0           layer_2/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_2/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1_add (Add)          (None, 512, 1024)    0           layer_1/ln_2[0][0]               \n",
      "                                                                 layer_2/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_2/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/gelu (Gelu)             (None, 512, 4096)    0           layer_2/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_2/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_2/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2_add (Add)          (None, 512, 1024)    0           layer_2/ln_1[0][0]               \n",
      "                                                                 layer_2/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_2/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_2/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/self_attention (MultiHe (None, 512, 1024)    0           layer_3/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_3/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1_add (Add)          (None, 512, 1024)    0           layer_2/ln_2[0][0]               \n",
      "                                                                 layer_3/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_3/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/gelu (Gelu)             (None, 512, 4096)    0           layer_3/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_3/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_3/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2_add (Add)          (None, 512, 1024)    0           layer_3/ln_1[0][0]               \n",
      "                                                                 layer_3/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_3/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_3/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_3/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/self_attention (MultiHe (None, 512, 1024)    0           layer_4/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_4/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1_add (Add)          (None, 512, 1024)    0           layer_3/ln_2[0][0]               \n",
      "                                                                 layer_4/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_4/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/gelu (Gelu)             (None, 512, 4096)    0           layer_4/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_4/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_4/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2_add (Add)          (None, 512, 1024)    0           layer_4/ln_1[0][0]               \n",
      "                                                                 layer_4/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_4/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_4/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_4/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/self_attention (MultiHe (None, 512, 1024)    0           layer_5/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_5/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1_add (Add)          (None, 512, 1024)    0           layer_4/ln_2[0][0]               \n",
      "                                                                 layer_5/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_5/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/gelu (Gelu)             (None, 512, 4096)    0           layer_5/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_5/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_5/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2_add (Add)          (None, 512, 1024)    0           layer_5/ln_1[0][0]               \n",
      "                                                                 layer_5/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_5/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_5/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_5/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/self_attention (MultiHe (None, 512, 1024)    0           layer_6/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_6/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1_add (Add)          (None, 512, 1024)    0           layer_5/ln_2[0][0]               \n",
      "                                                                 layer_6/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_6/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/gelu (Gelu)             (None, 512, 4096)    0           layer_6/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_6/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_6/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2_add (Add)          (None, 512, 1024)    0           layer_6/ln_1[0][0]               \n",
      "                                                                 layer_6/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_6/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_6/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_6/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/self_attention (MultiHe (None, 512, 1024)    0           layer_7/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_7/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1_add (Add)          (None, 512, 1024)    0           layer_6/ln_2[0][0]               \n",
      "                                                                 layer_7/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_7/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/gelu (Gelu)             (None, 512, 4096)    0           layer_7/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_7/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_7/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2_add (Add)          (None, 512, 1024)    0           layer_7/ln_1[0][0]               \n",
      "                                                                 layer_7/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_7/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_7/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_7/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/self_attention (MultiHe (None, 512, 1024)    0           layer_8/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_8/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1_add (Add)          (None, 512, 1024)    0           layer_7/ln_2[0][0]               \n",
      "                                                                 layer_8/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_8/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/gelu (Gelu)             (None, 512, 4096)    0           layer_8/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_8/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_8/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2_add (Add)          (None, 512, 1024)    0           layer_8/ln_1[0][0]               \n",
      "                                                                 layer_8/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_8/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_8/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn (Conv1D)         (None, 512, 3072)    3148800     layer_8/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/self_attention (MultiHe (None, 512, 1024)    0           layer_9/c_attn[0][0]             \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_attn_proj (Conv1D)    (None, 512, 1024)    1049600     layer_9/self_attention[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_attn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1_add (Add)          (None, 512, 1024)    0           layer_8/ln_2[0][0]               \n",
      "                                                                 layer_9/ln_1_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_1 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_fc (Conv1D)           (None, 512, 4096)    4198400     layer_9/ln_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/gelu (Gelu)             (None, 512, 4096)    0           layer_9/c_fc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/c_ffn_proj (Conv1D)     (None, 512, 1024)    4195328     layer_9/gelu[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_drop (Dropout)     (None, 512, 1024)    0           layer_9/c_ffn_proj[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2_add (Add)          (None, 512, 1024)    0           layer_9/ln_1[0][0]               \n",
      "                                                                 layer_9/ln_2_drop[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_9/ln_2 (LayerNormalizatio (None, 512, 1024)    2048        layer_9/ln_2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_9/ln_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/self_attention (MultiH (None, 512, 1024)    0           layer_10/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_10/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1_add (Add)         (None, 512, 1024)    0           layer_9/ln_2[0][0]               \n",
      "                                                                 layer_10/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_10/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/gelu (Gelu)            (None, 512, 4096)    0           layer_10/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_10/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_10/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2_add (Add)         (None, 512, 1024)    0           layer_10/ln_1[0][0]              \n",
      "                                                                 layer_10/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_10/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_10/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_10/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/self_attention (MultiH (None, 512, 1024)    0           layer_11/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_11/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1_add (Add)         (None, 512, 1024)    0           layer_10/ln_2[0][0]              \n",
      "                                                                 layer_11/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_11/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/gelu (Gelu)            (None, 512, 4096)    0           layer_11/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_11/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_11/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2_add (Add)         (None, 512, 1024)    0           layer_11/ln_1[0][0]              \n",
      "                                                                 layer_11/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_11/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_11/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_11/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/self_attention (MultiH (None, 512, 1024)    0           layer_12/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_12/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1_add (Add)         (None, 512, 1024)    0           layer_11/ln_2[0][0]              \n",
      "                                                                 layer_12/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_12/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/gelu (Gelu)            (None, 512, 4096)    0           layer_12/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_12/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_12/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2_add (Add)         (None, 512, 1024)    0           layer_12/ln_1[0][0]              \n",
      "                                                                 layer_12/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_12/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_12/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_12/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/self_attention (MultiH (None, 512, 1024)    0           layer_13/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_13/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1_add (Add)         (None, 512, 1024)    0           layer_12/ln_2[0][0]              \n",
      "                                                                 layer_13/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_13/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/gelu (Gelu)            (None, 512, 4096)    0           layer_13/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_13/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_13/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2_add (Add)         (None, 512, 1024)    0           layer_13/ln_1[0][0]              \n",
      "                                                                 layer_13/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_13/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_13/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_13/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/self_attention (MultiH (None, 512, 1024)    0           layer_14/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_14/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1_add (Add)         (None, 512, 1024)    0           layer_13/ln_2[0][0]              \n",
      "                                                                 layer_14/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_14/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/gelu (Gelu)            (None, 512, 4096)    0           layer_14/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_14/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_14/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2_add (Add)         (None, 512, 1024)    0           layer_14/ln_1[0][0]              \n",
      "                                                                 layer_14/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_14/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_14/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_14/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/self_attention (MultiH (None, 512, 1024)    0           layer_15/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_15/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1_add (Add)         (None, 512, 1024)    0           layer_14/ln_2[0][0]              \n",
      "                                                                 layer_15/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_15/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/gelu (Gelu)            (None, 512, 4096)    0           layer_15/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_15/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_15/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2_add (Add)         (None, 512, 1024)    0           layer_15/ln_1[0][0]              \n",
      "                                                                 layer_15/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_15/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_15/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_15/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/self_attention (MultiH (None, 512, 1024)    0           layer_16/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_16/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1_add (Add)         (None, 512, 1024)    0           layer_15/ln_2[0][0]              \n",
      "                                                                 layer_16/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_16/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/gelu (Gelu)            (None, 512, 4096)    0           layer_16/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_16/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_16/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2_add (Add)         (None, 512, 1024)    0           layer_16/ln_1[0][0]              \n",
      "                                                                 layer_16/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_16/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_16/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_16/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/self_attention (MultiH (None, 512, 1024)    0           layer_17/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_17/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1_add (Add)         (None, 512, 1024)    0           layer_16/ln_2[0][0]              \n",
      "                                                                 layer_17/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_17/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/gelu (Gelu)            (None, 512, 4096)    0           layer_17/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_17/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_17/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2_add (Add)         (None, 512, 1024)    0           layer_17/ln_1[0][0]              \n",
      "                                                                 layer_17/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_17/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_17/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_17/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/self_attention (MultiH (None, 512, 1024)    0           layer_18/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_18/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1_add (Add)         (None, 512, 1024)    0           layer_17/ln_2[0][0]              \n",
      "                                                                 layer_18/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_18/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/gelu (Gelu)            (None, 512, 4096)    0           layer_18/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_18/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_18/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2_add (Add)         (None, 512, 1024)    0           layer_18/ln_1[0][0]              \n",
      "                                                                 layer_18/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_18/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_18/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_18/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/self_attention (MultiH (None, 512, 1024)    0           layer_19/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_19/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1_add (Add)         (None, 512, 1024)    0           layer_18/ln_2[0][0]              \n",
      "                                                                 layer_19/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_19/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/gelu (Gelu)            (None, 512, 4096)    0           layer_19/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_19/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_19/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2_add (Add)         (None, 512, 1024)    0           layer_19/ln_1[0][0]              \n",
      "                                                                 layer_19/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_19/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_19/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_19/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/self_attention (MultiH (None, 512, 1024)    0           layer_20/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_20/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1_add (Add)         (None, 512, 1024)    0           layer_19/ln_2[0][0]              \n",
      "                                                                 layer_20/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_20/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/gelu (Gelu)            (None, 512, 4096)    0           layer_20/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_20/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_20/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2_add (Add)         (None, 512, 1024)    0           layer_20/ln_1[0][0]              \n",
      "                                                                 layer_20/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_20/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_20/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_20/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/self_attention (MultiH (None, 512, 1024)    0           layer_21/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_21/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1_add (Add)         (None, 512, 1024)    0           layer_20/ln_2[0][0]              \n",
      "                                                                 layer_21/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_21/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/gelu (Gelu)            (None, 512, 4096)    0           layer_21/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_21/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_21/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2_add (Add)         (None, 512, 1024)    0           layer_21/ln_1[0][0]              \n",
      "                                                                 layer_21/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_21/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_21/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_21/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/self_attention (MultiH (None, 512, 1024)    0           layer_22/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_22/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1_add (Add)         (None, 512, 1024)    0           layer_21/ln_2[0][0]              \n",
      "                                                                 layer_22/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_22/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/gelu (Gelu)            (None, 512, 4096)    0           layer_22/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_22/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_22/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2_add (Add)         (None, 512, 1024)    0           layer_22/ln_1[0][0]              \n",
      "                                                                 layer_22/ln_2_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_22/ln_2 (LayerNormalizati (None, 512, 1024)    2048        layer_22/ln_2_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn (Conv1D)        (None, 512, 3072)    3148800     layer_22/ln_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/self_attention (MultiH (None, 512, 1024)    0           layer_23/c_attn[0][0]            \n",
      "                                                                 attention_mask_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_attn_proj (Conv1D)   (None, 512, 1024)    1049600     layer_23/self_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_attn_proj[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1_add (Add)         (None, 512, 1024)    0           layer_22/ln_2[0][0]              \n",
      "                                                                 layer_23/ln_1_drop[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_1 (LayerNormalizati (None, 512, 1024)    2048        layer_23/ln_1_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_fc (Conv1D)          (None, 512, 4096)    4198400     layer_23/ln_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/gelu (Gelu)            (None, 512, 4096)    0           layer_23/c_fc[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/c_ffn_proj (Conv1D)    (None, 512, 1024)    4195328     layer_23/gelu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_drop (Dropout)    (None, 512, 1024)    0           layer_23/c_ffn_proj[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_23/ln_2_add (Add)         (None, 512, 1024)    0           layer_23/ln_1[0][0]              \n",
      "                                                                 layer_23/ln_2_drop[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 332,427,264\n",
      "Trainable params: 0\n",
      "Non-trainable params: 332,427,264\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_model.build(None)\n",
    "bert_model._layers.pop()\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-522a83368298>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "bert_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
